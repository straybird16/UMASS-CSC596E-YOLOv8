{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from IPython.display import Image  # for displaying images\n",
    "import os \n",
    "import random\n",
    "from xml.dom import minidom\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from utils import *\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bboxes': [{'class': 'trafficlight', 'xmin': 20, 'ymin': 109, 'xmax': 81, 'ymax': 237}, {'class': 'trafficlight', 'xmin': 116, 'ymin': 162, 'xmax': 163, 'ymax': 272}, {'class': 'trafficlight', 'xmin': 189, 'ymin': 189, 'xmax': 233, 'ymax': 295}], 'filename': 'road4.png', 'image_size': (267, 400, 3)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:00<00:00, 4730.22it/s]\n"
     ]
    }
   ],
   "source": [
    "print(extract_info_from_xml('temp_data/annotations/road4.xml'))\n",
    "path_annotations = 'temp_data/annotations'\n",
    "# Get the annotations\n",
    "annotations = [os.path.join(path_annotations, x) for x in os.listdir(path_annotations) if x[-3:] == \"xml\"]\n",
    "annotations.sort()\n",
    "\n",
    "# Convert and save the annotations\n",
    "for ann in tqdm(annotations):\n",
    "    info_dict = extract_info_from_xml(ann)\n",
    "    convert_to_yolov5(info_dict)\n",
    "annotations = [os.path.join(path_annotations, x) for x in os.listdir(path_annotations) if x[-3:] == \"txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get any random annotation file \n",
    "annotation_file = random.choice(annotations)\n",
    "with open(annotation_file, \"r\") as file:\n",
    "    annotation_list = file.read().split(\"\\n\")[:-1]\n",
    "    annotation_list = [x.split(\" \") for x in annotation_list]\n",
    "    annotation_list = [[float(y) for y in x ] for x in annotation_list]\n",
    "\n",
    "#Get the corresponding image file\n",
    "image_file = annotation_file.replace(\"annotations\", \"images\").replace(\"txt\", \"png\")\n",
    "assert os.path.exists(image_file)\n",
    "\n",
    "#Load the image\n",
    "image = Image.open(image_file)\n",
    "\n",
    "#Plot the Bounding Box\n",
    "plot_bounding_box(image, annotation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_dataset(path_images='temp_data/images', path_annotations='temp_data/annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = YOLO('yolov8n.yaml')\n",
    "model = YOLO('yolov8n.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.59 ðŸš€ Python-3.10.10 torch-2.0.0 CPU\n",
      "Model summary (fused): 168 layers, 3006233 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/xxd/Documents/Course/CS596E/YOLOv8/datasets/full_dataset1/labels/val.cache... 200 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:24<00:00,  1.87s/it]\n",
      "                   all        200        600      0.982      0.668      0.912       0.76\n",
      "                   GUI        200        200      0.995          1      0.995      0.995\n",
      "            three dots        200        200          1     0.0128      0.754      0.472\n",
      "               textbar        200        200      0.952       0.99      0.986      0.812\n",
      "Speed: 0.7ms preprocess, 114.6ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('runs/detect/train9/weights/best.pt')\n",
    "# train\n",
    "#results = model.train(data='temp_data/temp_data.yaml', epochs=20, save=True)\n",
    "#results = myTrain(model, data='temp_data/temp_data.yaml', hyp='temp_data/hyp.txt')\n",
    "#results = myTrain(model, data='datasets/full_dataset1/full_dataset1.yaml', hyp='datasets/full_dataset1/hyp.txt')\n",
    "# validate\n",
    "results = model.val(data='datasets/full_dataset1/full_dataset1.yaml')\n",
    "# Perform object detection on an image using the model\n",
    "#results = model('https://ultralytics.com/images/bus.jpg')\n",
    "#res_plotted = results[0].plot()\n",
    "#cv2.imshow(\"result\", res_plotted)\n",
    "#cv2.waitKey(10000)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "#success = model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Found https://media.roboflow.com/notebooks/examples/dog.jpeg locally at dog.jpeg\n",
      "image 1/1 /Users/xxd/Documents/Course/CS596E/YOLOv8/dog.jpeg: 640x384 (no detections), 164.8ms\n",
      "Speed: 40.9ms preprocess, 164.8ms inference, 8.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Ultralytics YOLOv8.0.59 ðŸš€ Python-3.10.10 torch-2.0.0 CPU\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/xxd/Documents/Course/CS596E/YOLOv8/datasets/temp_data/labels/val.cache... 88 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:12<00:00,  2.08s/it]\n",
      "                   all         88        132      0.928      0.888      0.938      0.771\n",
      "          trafficlight         88         19      0.822      0.737      0.802      0.509\n",
      "                  stop         88         10      0.953        0.9      0.972      0.883\n",
      "            speedlimit         88         81      0.985          1      0.995      0.901\n",
      "             crosswalk         88         22      0.953      0.914      0.983       0.79\n",
      "Speed: 1.2ms preprocess, 132.9ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val4\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.yolo.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0, 1, 2, 3])\n",
       "box: ultralytics.yolo.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.yolo.utils.metrics.ConfusionMatrix object at 0x12477b2e0>\n",
       "fitness: 0.7874004571307239\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.50873,     0.88328,     0.90071,     0.78991])\n",
       "names: {0: 'trafficlight', 1: 'stop', 2: 'speedlimit', 3: 'crosswalk'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': 0.928441805668051, 'metrics/recall(B)': 0.8876650045370222, 'metrics/mAP50(B)': 0.9380756417707901, 'metrics/mAP50-95(B)': 0.7706587699484944, 'fitness': 0.7874004571307239}\n",
       "save_dir: PosixPath('runs/detect/val4')\n",
       "speed: {'preprocess': 1.17575851353732, 'inference': 132.88199088790202, 'loss': 0.0007179650393399324, 'postprocess': 0.27259642427617853}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = YOLO('yolov8n.pt')\n",
    "model.predict(\n",
    "   source='https://media.roboflow.com/notebooks/examples/dog.jpeg',\n",
    "   conf=0.255000\n",
    ")\n",
    "model.val(data='temp_data/temp_data.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d52ca74710e1134f80a6ecd73bcd1d0bffc2dd65bb7b82405303a70bd99d9de8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
